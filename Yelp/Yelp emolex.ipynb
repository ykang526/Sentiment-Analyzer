{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis is the sentiment analysis with\\n\\nemolex libray\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is the sentiment analysis with\n",
    "\n",
    "emolex libray\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.text import Text\n",
    "from nltk.classify import ClassifierI\n",
    "import string, re\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "\n",
    "documents = []\n",
    "all_words = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "                \n",
    "emolex = open(\"./NRC_Emotion.txt\", \"r\")\n",
    "pos = {}\n",
    "neg = {}\n",
    "for line in emolex:\n",
    "    words = line.strip()\n",
    "    words = words.split('\\t')\n",
    "    if 'positive' in words and '1' in words:\n",
    "        pos[words[0]] = bool(1)\n",
    "    if 'negative' in words and '1' in words:\n",
    "        neg[words[0]] = bool(1)\n",
    "emolex.close\n",
    "\n",
    "pos_featureset = [(pos, 'pos')]\n",
    "neg_featureset = [(neg, 'neg')]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "yelp = io.open(\"all_reviews.txt\", \"r\", encoding = 'utf-8_sig')\n",
    "pos = {}\n",
    "neg = {}\n",
    "\n",
    "reviews = yelp.readlines()\n",
    "reviews = [x.strip() for x in reviews] # seperate each line\n",
    "star_flag = bool(False)\n",
    "star = 0\n",
    "documents = []\n",
    "temp = ''\n",
    "for line in reviews:\n",
    "    if line == '{{{':\n",
    "        star_flag = bool(True)\n",
    "        continue\n",
    "        \n",
    "    if star_flag:\n",
    "        star = (int(re.findall(r'^[1-5]',line)[0]))\n",
    "        star_flag = bool(False)\n",
    "        continue\n",
    "        \n",
    "    if line == '':\n",
    "        continue\n",
    "        \n",
    "    if line == '}}}':\n",
    "        continue\n",
    "        \n",
    "    if line == '[[[':\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        if line == ']]]':\n",
    "            temp = temp.replace('\\xa0','') #some wierd encoding error stuff...\n",
    "            words = tokenizer.tokenize(temp)\n",
    "            \n",
    "            #determine if the review is negative or positive and append it to documents list\n",
    "            if star > 3:\n",
    "                documents.append((words, 'pos'))\n",
    "            else:\n",
    "                documents.append((words, 'neg'))\n",
    "            \n",
    "            #add the tokenized words to bag of words\n",
    "\n",
    "            all_words.extend(words)\n",
    "            \n",
    "            #reset temp\n",
    "            temp = ''\n",
    "        else:\n",
    "            temp += line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that finds features from a document\n",
    "#features are top 20% most frequent words from all documents\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "top_10 = int(len(all_words)*.20)\n",
    "word_features, frequency = map(list, zip(*all_words.most_common(top_10)))\n",
    "\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10391\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(find_features(review), category) for (review, category) in documents]\n",
    "random.shuffle(featuresets)\n",
    "print(len(featuresets))\n",
    "first75 = int(len(featuresets)*.75)\n",
    "\n",
    "training_set = featuresets[:first75] + pos_featureset + neg_featureset\n",
    "testing_set = featuresets[first75:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now classifying MultinomialNB\n",
      "MultinomialNB classifier accuracy: 0.8406466512702079\n",
      "\n",
      "\n",
      "Now classifying BernoulliNB\n",
      "BernoulliNB classifier accuracy: 0.7263279445727483\n",
      "\n",
      "\n",
      "Now classifying Decision_Tree\n",
      "Decision_Tree classifier accuracy: 0.7806004618937644\n",
      "\n",
      "\n",
      "Now classifying Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YunMo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression classifier accuracy: 0.8648960739030023\n",
      "\n",
      "\n",
      "Now classifying SVC\n",
      "SVC classifier accuracy: 0.8710546574287914\n",
      "\n",
      "\n",
      "Now classifying LinearSVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YunMo\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC classifier accuracy: 0.8456505003849115\n",
      "\n",
      "\n",
      "Now classifying NuSVC\n",
      "NuSVC classifier accuracy: 0.8706697459584296\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = ['MultinomialNB','BernoulliNB', \n",
    "         'Decision_Tree',\n",
    "         'Logistic Regression',\n",
    "         'SVC', 'LinearSVC', 'NuSVC']\n",
    "\n",
    "trained = []\n",
    "\n",
    "classifiers = [MultinomialNB(), BernoulliNB(), \n",
    "               DecisionTreeClassifier(random_state=0), \n",
    "               LogisticRegression(),\n",
    "               SVC(), LinearSVC(), NuSVC()]\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    print('Now classifying', name)\n",
    "    \n",
    "    classifier = SklearnClassifier(clf)\n",
    "    classifier.train(training_set)\n",
    "    trained.append(classifier)\n",
    "    print(\"{} classifier accuracy:\".format(name), nltk.classify.accuracy(classifier, testing_set))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(trained, input_text):\n",
    "    text = tokenizer.tokenize(input_text)\n",
    "    features = find_features(text)\n",
    "\n",
    "    return trained[3].classify(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your input's sentiment is:  pos\n"
     ]
    }
   ],
   "source": [
    "#input the text you want sentiment analyzed in here:\n",
    "#after inputting, please press shift + enter to run this cell.\n",
    "\n",
    "sentiment_text = \"This film was awesome, it took me back to when I was a child playing with my millennium falcon at home. It's fan service at its purest and repaired all the damage from the last film. Don't watch this film with a predetermined view of how you think this film should be, and watch it through a child's eyes as you did the first time you watched this franchise and just enjoy.\"\n",
    "\n",
    "print(\"your input's sentiment is: \", analyze_text(trained, sentiment_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
